apiVersion: batch/v1
kind: Job
metadata:
  name: promptloader
  namespace: llm-servings
  labels:
    app: promptloader
spec:
  template:
    metadata:
      labels:
        app: promptloader
    spec:
      restartPolicy: Never
      containers:
      - name: python-runner
        image: python:3.9-slim
        command: [“/bin/bash”]
        args:
          - “-c”
          - |
            cd /workspace
            pip install requests
            cp /app/promptload.py /workspace/promptload.py
            cp /prompts-data/prompts.txt /workspace/prompts.txt
            python3 promptload.py --parallels 1 --prompts_per_worker 1 --min_tokens 3000 --max_tokens 3000 --sleep_ms 50 --duration 1 --log debug --prompt_files prompts.txt
        env:
        - name: MODEL
          value: “meta-llama/Llama-3.1-8B”
        volumeMounts:
        - name: python-code
          mountPath: /app
        - name: prompts-data
          mountPath: /prompts-data
        - name: workspace-data
          mountPath: /workspace
        resources:
          requests:
            cpu: “100m”
            memory: “256Mi”
          limits:
            cpu: “500m”
            memory: “512Mi”
      volumes:
      - name: python-code
        configMap:
          name: pythoncode
          defaultMode: 0755
      - name: prompts-data
        configMap:
          name: prompts
      - name: workspace-data
        persistentVolumeClaim:
          claimName: tmp-space
  backoffLimit: 3
